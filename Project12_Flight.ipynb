{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test=pd.read_excel('E:/flight_Test.xlsx',parse_dates=['Date_of_Journey'])\n",
    "df_train=pd.read_excel('E:/flight_Train.xlsx',parse_dates=['Date_of_Journey'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flight=df_train.append(df_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Airline</th>\n",
       "      <th>Date_of_Journey</th>\n",
       "      <th>Source</th>\n",
       "      <th>Destination</th>\n",
       "      <th>Route</th>\n",
       "      <th>Dep_Time</th>\n",
       "      <th>Arrival_Time</th>\n",
       "      <th>Duration</th>\n",
       "      <th>Total_Stops</th>\n",
       "      <th>Additional_Info</th>\n",
       "      <th>Price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>IndiGo</td>\n",
       "      <td>2019-03-24</td>\n",
       "      <td>Banglore</td>\n",
       "      <td>New Delhi</td>\n",
       "      <td>BLR → DEL</td>\n",
       "      <td>22:20</td>\n",
       "      <td>01:10 22 Mar</td>\n",
       "      <td>2h 50m</td>\n",
       "      <td>non-stop</td>\n",
       "      <td>No info</td>\n",
       "      <td>3897.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Air India</td>\n",
       "      <td>2019-01-05</td>\n",
       "      <td>Kolkata</td>\n",
       "      <td>Banglore</td>\n",
       "      <td>CCU → IXR → BBI → BLR</td>\n",
       "      <td>05:50</td>\n",
       "      <td>13:15</td>\n",
       "      <td>7h 25m</td>\n",
       "      <td>2 stops</td>\n",
       "      <td>No info</td>\n",
       "      <td>7662.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Jet Airways</td>\n",
       "      <td>2019-09-06</td>\n",
       "      <td>Delhi</td>\n",
       "      <td>Cochin</td>\n",
       "      <td>DEL → LKO → BOM → COK</td>\n",
       "      <td>09:25</td>\n",
       "      <td>04:25 10 Jun</td>\n",
       "      <td>19h</td>\n",
       "      <td>2 stops</td>\n",
       "      <td>No info</td>\n",
       "      <td>13882.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>IndiGo</td>\n",
       "      <td>2019-12-05</td>\n",
       "      <td>Kolkata</td>\n",
       "      <td>Banglore</td>\n",
       "      <td>CCU → NAG → BLR</td>\n",
       "      <td>18:05</td>\n",
       "      <td>23:30</td>\n",
       "      <td>5h 25m</td>\n",
       "      <td>1 stop</td>\n",
       "      <td>No info</td>\n",
       "      <td>6218.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>IndiGo</td>\n",
       "      <td>2019-01-03</td>\n",
       "      <td>Banglore</td>\n",
       "      <td>New Delhi</td>\n",
       "      <td>BLR → NAG → DEL</td>\n",
       "      <td>16:50</td>\n",
       "      <td>21:35</td>\n",
       "      <td>4h 45m</td>\n",
       "      <td>1 stop</td>\n",
       "      <td>No info</td>\n",
       "      <td>13302.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2666</th>\n",
       "      <td>Air India</td>\n",
       "      <td>2019-06-06</td>\n",
       "      <td>Kolkata</td>\n",
       "      <td>Banglore</td>\n",
       "      <td>CCU → DEL → BLR</td>\n",
       "      <td>20:30</td>\n",
       "      <td>20:25 07 Jun</td>\n",
       "      <td>23h 55m</td>\n",
       "      <td>1 stop</td>\n",
       "      <td>No info</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2667</th>\n",
       "      <td>IndiGo</td>\n",
       "      <td>2019-03-27</td>\n",
       "      <td>Kolkata</td>\n",
       "      <td>Banglore</td>\n",
       "      <td>CCU → BLR</td>\n",
       "      <td>14:20</td>\n",
       "      <td>16:55</td>\n",
       "      <td>2h 35m</td>\n",
       "      <td>non-stop</td>\n",
       "      <td>No info</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2668</th>\n",
       "      <td>Jet Airways</td>\n",
       "      <td>2019-06-03</td>\n",
       "      <td>Delhi</td>\n",
       "      <td>Cochin</td>\n",
       "      <td>DEL → BOM → COK</td>\n",
       "      <td>21:50</td>\n",
       "      <td>04:25 07 Mar</td>\n",
       "      <td>6h 35m</td>\n",
       "      <td>1 stop</td>\n",
       "      <td>No info</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2669</th>\n",
       "      <td>Air India</td>\n",
       "      <td>2019-06-03</td>\n",
       "      <td>Delhi</td>\n",
       "      <td>Cochin</td>\n",
       "      <td>DEL → BOM → COK</td>\n",
       "      <td>04:00</td>\n",
       "      <td>19:15</td>\n",
       "      <td>15h 15m</td>\n",
       "      <td>1 stop</td>\n",
       "      <td>No info</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2670</th>\n",
       "      <td>Multiple carriers</td>\n",
       "      <td>2019-06-15</td>\n",
       "      <td>Delhi</td>\n",
       "      <td>Cochin</td>\n",
       "      <td>DEL → BOM → COK</td>\n",
       "      <td>04:55</td>\n",
       "      <td>19:15</td>\n",
       "      <td>14h 20m</td>\n",
       "      <td>1 stop</td>\n",
       "      <td>No info</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>13354 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                Airline Date_of_Journey    Source Destination  \\\n",
       "0                IndiGo      2019-03-24  Banglore   New Delhi   \n",
       "1             Air India      2019-01-05   Kolkata    Banglore   \n",
       "2           Jet Airways      2019-09-06     Delhi      Cochin   \n",
       "3                IndiGo      2019-12-05   Kolkata    Banglore   \n",
       "4                IndiGo      2019-01-03  Banglore   New Delhi   \n",
       "...                 ...             ...       ...         ...   \n",
       "2666          Air India      2019-06-06   Kolkata    Banglore   \n",
       "2667             IndiGo      2019-03-27   Kolkata    Banglore   \n",
       "2668        Jet Airways      2019-06-03     Delhi      Cochin   \n",
       "2669          Air India      2019-06-03     Delhi      Cochin   \n",
       "2670  Multiple carriers      2019-06-15     Delhi      Cochin   \n",
       "\n",
       "                      Route Dep_Time  Arrival_Time Duration Total_Stops  \\\n",
       "0                 BLR → DEL    22:20  01:10 22 Mar   2h 50m    non-stop   \n",
       "1     CCU → IXR → BBI → BLR    05:50         13:15   7h 25m     2 stops   \n",
       "2     DEL → LKO → BOM → COK    09:25  04:25 10 Jun      19h     2 stops   \n",
       "3           CCU → NAG → BLR    18:05         23:30   5h 25m      1 stop   \n",
       "4           BLR → NAG → DEL    16:50         21:35   4h 45m      1 stop   \n",
       "...                     ...      ...           ...      ...         ...   \n",
       "2666        CCU → DEL → BLR    20:30  20:25 07 Jun  23h 55m      1 stop   \n",
       "2667              CCU → BLR    14:20         16:55   2h 35m    non-stop   \n",
       "2668        DEL → BOM → COK    21:50  04:25 07 Mar   6h 35m      1 stop   \n",
       "2669        DEL → BOM → COK    04:00         19:15  15h 15m      1 stop   \n",
       "2670        DEL → BOM → COK    04:55         19:15  14h 20m      1 stop   \n",
       "\n",
       "     Additional_Info    Price  \n",
       "0            No info   3897.0  \n",
       "1            No info   7662.0  \n",
       "2            No info  13882.0  \n",
       "3            No info   6218.0  \n",
       "4            No info  13302.0  \n",
       "...              ...      ...  \n",
       "2666         No info      NaN  \n",
       "2667         No info      NaN  \n",
       "2668         No info      NaN  \n",
       "2669         No info      NaN  \n",
       "2670         No info      NaN  \n",
       "\n",
       "[13354 rows x 11 columns]"
      ]
     },
     "execution_count": 429,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_flight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x21bfdda1fc8>"
      ]
     },
     "execution_count": 430,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#checking for any missing data\n",
    "sns.heatmap(df_flight.isnull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Airline               0\n",
       "Date_of_Journey       0\n",
       "Source                0\n",
       "Destination           0\n",
       "Route                 1\n",
       "Dep_Time              0\n",
       "Arrival_Time          0\n",
       "Duration              0\n",
       "Total_Stops           1\n",
       "Additional_Info       0\n",
       "Price              2671\n",
       "dtype: int64"
      ]
     },
     "execution_count": 431,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_flight.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flight['month']=df_flight['Date_of_Journey'].dt.month_name()\n",
    "df_flight['year']=df_flight['Date_of_Journey'].dt.year\n",
    "df_flight['day']=df_flight['Date_of_Journey'].dt.day_name()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 433,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flight['Arrival_Time']=df_flight['Arrival_Time'].str.split(' ').str[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flight['Arrival_Hour']=df_flight['Arrival_Time'].str.split(':').str[0]\n",
    "df_flight['Arrival_Minute']=df_flight['Arrival_Time'].str.split(':').str[1]\n",
    "\n",
    "df_flight['Arrival_Hour']=df_flight['Arrival_Hour'].astype(int)\n",
    "df_flight['Arrival_Minute']=df_flight['Arrival_Minute'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flight['Total_Stops']=df_flight['Total_Stops'].replace('non-stop',int(0))\n",
    "df_flight['Total_Stops']=df_flight['Total_Stops'].replace('1 stop',1)\n",
    "df_flight['Total_Stops']=df_flight['Total_Stops'].replace('2 stops',2)\n",
    "df_flight['Total_Stops']=df_flight['Total_Stops'].replace('3 stops',3)\n",
    "df_flight['Total_Stops']=df_flight['Total_Stops'].replace('4 stops',4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flight['Total_Stops'].dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flight['Arrival_Hour']=df_flight['Arrival_Time'].str.split(':').str[0]\n",
    "df_flight['Arrival_Minute']=df_flight['Arrival_Time'].str.split(':').str[1]\n",
    "\n",
    "df_flight['Arrival_Hour']=df_flight['Arrival_Hour'].astype(int)\n",
    "df_flight['Arrival_Minute']=df_flight['Arrival_Minute'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flight['Dep_Hour']=df_flight['Dep_Time'].str.split(':').str[0]\n",
    "df_flight['Dep_Minute']=df_flight['Dep_Time'].str.split(':').str[1]\n",
    "\n",
    "df_flight['Dep_Hour']=df_flight['Dep_Hour'].astype(int)\n",
    "df_flight['Dep_Minute']=df_flight['Dep_Minute'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 439,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flight['Route_1']=df_flight['Route'].str.split('→ ').str[0]\n",
    "df_flight['Route_2']=df_flight['Route'].str.split('→ ').str[1]\n",
    "df_flight['Route_3']=df_flight['Route'].str.split('→ ').str[2]\n",
    "df_flight['Route_4']=df_flight['Route'].str.split('→ ').str[3]\n",
    "df_flight['Route_5']=df_flight['Route'].str.split('→ ').str[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 440,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flight['Route_1'].fillna('None',inplace=True)\n",
    "df_flight['Route_2'].fillna('None',inplace=True)\n",
    "df_flight['Route_3'].fillna('None',inplace=True)\n",
    "df_flight['Route_4'].fillna('None',inplace=True)\n",
    "df_flight['Route_5'].fillna('None',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 441,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "df_flight[\"Additional_Info\"] = le.fit_transform(df_flight[\"Additional_Info\"])\n",
    "df_flight[\"Airline\"] = le.fit_transform(df_flight[\"Airline\"])\n",
    "df_flight[\"Destination\"] = le.fit_transform(df_flight[\"Destination\"])\n",
    "df_flight[\"Source\"] = le.fit_transform(df_flight[\"Source\"])\n",
    "df_flight[\"month\"] = le.fit_transform(df_flight[\"month\"])\n",
    "df_flight[\"day\"] = le.fit_transform(df_flight[\"day\"])\n",
    "df_flight[\"Route_1\"] = le.fit_transform(df_flight[\"Route_1\"])\n",
    "df_flight[\"Route_2\"] = le.fit_transform(df_flight[\"Route_2\"])\n",
    "df_flight[\"Route_3\"] = le.fit_transform(df_flight[\"Route_3\"])\n",
    "df_flight[\"Route_4\"] = le.fit_transform(df_flight[\"Route_4\"])\n",
    "df_flight[\"Route_5\"] = le.fit_transform(df_flight[\"Route_5\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Airline', 'Date_of_Journey', 'Source', 'Destination', 'Route',\n",
       "       'Dep_Time', 'Arrival_Time', 'Duration', 'Total_Stops',\n",
       "       'Additional_Info', 'Price', 'month', 'year', 'day', 'Arrival_Hour',\n",
       "       'Arrival_Minute', 'Dep_Hour', 'Dep_Minute', 'Route_1', 'Route_2',\n",
       "       'Route_3', 'Route_4', 'Route_5'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 442,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_flight.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flight.drop(['Date_of_Journey','Dep_Time','year','Arrival_Time','Duration','Route'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Airline', 'Source', 'Destination', 'Total_Stops', 'Additional_Info',\n",
       "       'Price', 'month', 'day', 'Arrival_Hour', 'Arrival_Minute', 'Dep_Hour',\n",
       "       'Dep_Minute', 'Route_1', 'Route_2', 'Route_3', 'Route_4', 'Route_5'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 448,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_flight.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_flight.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train=df_flight[0:10683]\n",
    "df_test=df_flight[10683:]\n",
    "df_test.drop(['Price'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 451,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Airline</th>\n",
       "      <th>Source</th>\n",
       "      <th>Destination</th>\n",
       "      <th>Total_Stops</th>\n",
       "      <th>Additional_Info</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>Arrival_Hour</th>\n",
       "      <th>Arrival_Minute</th>\n",
       "      <th>Dep_Hour</th>\n",
       "      <th>Dep_Minute</th>\n",
       "      <th>Route_1</th>\n",
       "      <th>Route_2</th>\n",
       "      <th>Route_3</th>\n",
       "      <th>Route_4</th>\n",
       "      <th>Route_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Airline, Source, Destination, Total_Stops, Additional_Info, month, day, Arrival_Hour, Arrival_Minute, Dep_Hour, Dep_Minute, Route_1, Route_2, Route_3, Route_4, Route_5]\n",
       "Index: []"
      ]
     },
     "execution_count": 451,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 452,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Airline</th>\n",
       "      <th>Source</th>\n",
       "      <th>Destination</th>\n",
       "      <th>Total_Stops</th>\n",
       "      <th>Additional_Info</th>\n",
       "      <th>Price</th>\n",
       "      <th>month</th>\n",
       "      <th>day</th>\n",
       "      <th>Arrival_Hour</th>\n",
       "      <th>Arrival_Minute</th>\n",
       "      <th>Dep_Hour</th>\n",
       "      <th>Dep_Minute</th>\n",
       "      <th>Route_1</th>\n",
       "      <th>Route_2</th>\n",
       "      <th>Route_3</th>\n",
       "      <th>Route_4</th>\n",
       "      <th>Route_5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>3897.0</td>\n",
       "      <td>4</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>22</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>24</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8</td>\n",
       "      <td>7662.0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>15</td>\n",
       "      <td>5</td>\n",
       "      <td>50</td>\n",
       "      <td>2</td>\n",
       "      <td>25</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8</td>\n",
       "      <td>13882.0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>25</td>\n",
       "      <td>9</td>\n",
       "      <td>25</td>\n",
       "      <td>3</td>\n",
       "      <td>32</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8</td>\n",
       "      <td>6218.0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>23</td>\n",
       "      <td>30</td>\n",
       "      <td>18</td>\n",
       "      <td>5</td>\n",
       "      <td>2</td>\n",
       "      <td>34</td>\n",
       "      <td>3</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1.0</td>\n",
       "      <td>8</td>\n",
       "      <td>13302.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>21</td>\n",
       "      <td>35</td>\n",
       "      <td>16</td>\n",
       "      <td>50</td>\n",
       "      <td>0</td>\n",
       "      <td>34</td>\n",
       "      <td>8</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10678</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>4107.0</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>22</td>\n",
       "      <td>25</td>\n",
       "      <td>19</td>\n",
       "      <td>55</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>24</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10679</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>4145.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>23</td>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>45</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>24</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10680</th>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>7229.0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>11</td>\n",
       "      <td>20</td>\n",
       "      <td>8</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>24</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10681</th>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8</td>\n",
       "      <td>12648.0</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>14</td>\n",
       "      <td>10</td>\n",
       "      <td>11</td>\n",
       "      <td>30</td>\n",
       "      <td>0</td>\n",
       "      <td>13</td>\n",
       "      <td>24</td>\n",
       "      <td>12</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10682</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>8</td>\n",
       "      <td>11753.0</td>\n",
       "      <td>6</td>\n",
       "      <td>4</td>\n",
       "      <td>19</td>\n",
       "      <td>15</td>\n",
       "      <td>10</td>\n",
       "      <td>55</td>\n",
       "      <td>3</td>\n",
       "      <td>16</td>\n",
       "      <td>4</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10682 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Airline  Source  Destination  Total_Stops  Additional_Info    Price  \\\n",
       "0            3       0            5          0.0                8   3897.0   \n",
       "1            1       3            0          2.0                8   7662.0   \n",
       "2            4       2            1          2.0                8  13882.0   \n",
       "3            3       3            0          1.0                8   6218.0   \n",
       "4            3       0            5          1.0                8  13302.0   \n",
       "...        ...     ...          ...          ...              ...      ...   \n",
       "10678        0       3            0          0.0                8   4107.0   \n",
       "10679        1       3            0          0.0                8   4145.0   \n",
       "10680        4       0            2          0.0                8   7229.0   \n",
       "10681       10       0            5          0.0                8  12648.0   \n",
       "10682        1       2            1          2.0                8  11753.0   \n",
       "\n",
       "       month  day  Arrival_Hour  Arrival_Minute  Dep_Hour  Dep_Minute  \\\n",
       "0          4    3             1              10        22          20   \n",
       "1          2    2            13              15         5          50   \n",
       "2          6    0             4              25         9          25   \n",
       "3          1    4            23              30        18           5   \n",
       "4          2    4            21              35        16          50   \n",
       "...      ...  ...           ...             ...       ...         ...   \n",
       "10678      6    6            22              25        19          55   \n",
       "10679      0    2            23              20        20          45   \n",
       "10680      0    2            11              20         8          20   \n",
       "10681      2    4            14              10        11          30   \n",
       "10682      6    4            19              15        10          55   \n",
       "\n",
       "       Route_1  Route_2  Route_3  Route_4  Route_5  \n",
       "0            0       13       24       12        4  \n",
       "1            2       25        1        3        4  \n",
       "2            3       32        4        5        4  \n",
       "3            2       34        3       12        4  \n",
       "4            0       34        8       12        4  \n",
       "...        ...      ...      ...      ...      ...  \n",
       "10678        2        5       24       12        4  \n",
       "10679        2        5       24       12        4  \n",
       "10680        0       13       24       12        4  \n",
       "10681        0       13       24       12        4  \n",
       "10682        3       16        4        5        4  \n",
       "\n",
       "[10682 rows x 17 columns]"
      ]
     },
     "execution_count": 452,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Airline             0.731057\n",
       "Source             -0.424023\n",
       "Destination         1.244046\n",
       "Total_Stops         0.317109\n",
       "Additional_Info    -1.779689\n",
       "Price               1.812405\n",
       "month              -0.340353\n",
       "day                -0.065171\n",
       "Arrival_Hour       -0.370146\n",
       "Arrival_Minute      0.110945\n",
       "Dep_Hour            0.112924\n",
       "Dep_Minute          0.167234\n",
       "Route_1            -0.618513\n",
       "Route_2             1.457157\n",
       "Route_3             0.582579\n",
       "Route_4            -2.103798\n",
       "Route_5           -16.440835\n",
       "dtype: float64"
      ]
     },
     "execution_count": 453,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.skew()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 462,
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing skewness using log transform\n",
    "for col in df_train.columns:\n",
    "    if df_train.skew().loc[col]>0.55:\n",
    "        df_train[col]=np.log1p(df_train[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 463,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.609438    3849\n",
      "1.386294    2053\n",
      "0.693147    1751\n",
      "1.945910    1196\n",
      "2.197225     818\n",
      "2.397895     479\n",
      "0.000000     319\n",
      "1.098612     194\n",
      "2.079442      13\n",
      "1.791759       6\n",
      "2.484907       3\n",
      "2.302585       1\n",
      "Name: Airline, dtype: int64\n",
      "2    4536\n",
      "3    2871\n",
      "0    2197\n",
      "4     697\n",
      "1     381\n",
      "Name: Source, dtype: int64\n",
      "0.693147    4536\n",
      "0.000000    2871\n",
      "1.098612    1265\n",
      "1.791759     932\n",
      "1.386294     697\n",
      "1.609438     381\n",
      "Name: Destination, dtype: int64\n",
      "1.0    5625\n",
      "0.0    3491\n",
      "2.0    1520\n",
      "3.0      45\n",
      "4.0       1\n",
      "Name: Total_Stops, dtype: int64\n",
      "8    8344\n",
      "5    1982\n",
      "7     320\n",
      "0      19\n",
      "4       7\n",
      "3       4\n",
      "6       3\n",
      "2       1\n",
      "9       1\n",
      "1       1\n",
      "Name: Additional_Info, dtype: int64\n",
      "9.236300    258\n",
      "9.291459    212\n",
      "8.885994    162\n",
      "8.477412    160\n",
      "8.481359    131\n",
      "           ... \n",
      "9.380842      1\n",
      "9.062652      1\n",
      "9.489335      1\n",
      "9.189219      1\n",
      "9.434603      1\n",
      "Name: Price, Length: 1870, dtype: int64\n",
      "3    2535\n",
      "4    2211\n",
      "5    2074\n",
      "6    1406\n",
      "2    1075\n",
      "1     957\n",
      "0     424\n",
      "Name: month, dtype: int64\n",
      "4    2290\n",
      "0    1802\n",
      "6    1598\n",
      "1    1491\n",
      "5    1236\n",
      "2    1203\n",
      "3    1062\n",
      "Name: day, dtype: int64\n",
      "19    1626\n",
      "12     897\n",
      "4      838\n",
      "21     703\n",
      "22     647\n",
      "1      529\n",
      "18     514\n",
      "9      489\n",
      "23     485\n",
      "10     476\n",
      "8      471\n",
      "7      417\n",
      "20     377\n",
      "16     370\n",
      "0      322\n",
      "13     308\n",
      "11     298\n",
      "14     295\n",
      "17     191\n",
      "15     182\n",
      "2       79\n",
      "5       69\n",
      "6       52\n",
      "3       47\n",
      "Name: Arrival_Hour, dtype: int64\n",
      "0     1447\n",
      "25    1301\n",
      "15    1286\n",
      "35    1111\n",
      "20     902\n",
      "30     832\n",
      "50     750\n",
      "45     697\n",
      "5      660\n",
      "40     629\n",
      "10     577\n",
      "55     490\n",
      "Name: Arrival_Minute, dtype: int64\n",
      "9     915\n",
      "7     867\n",
      "8     697\n",
      "17    695\n",
      "6     687\n",
      "20    651\n",
      "5     629\n",
      "11    580\n",
      "19    567\n",
      "10    536\n",
      "14    523\n",
      "21    492\n",
      "16    472\n",
      "18    444\n",
      "13    417\n",
      "22    387\n",
      "15    319\n",
      "2     194\n",
      "12    178\n",
      "4     170\n",
      "23    161\n",
      "0      40\n",
      "1      37\n",
      "3      24\n",
      "Name: Dep_Hour, dtype: int64\n",
      "0     2062\n",
      "30    1215\n",
      "55    1058\n",
      "10     890\n",
      "45     875\n",
      "5      773\n",
      "15     692\n",
      "25     691\n",
      "20     666\n",
      "35     665\n",
      "50     591\n",
      "40     504\n",
      "Name: Dep_Minute, dtype: int64\n",
      "3    4536\n",
      "2    2871\n",
      "0    2197\n",
      "1     697\n",
      "4     381\n",
      "Name: Route_1, dtype: int64\n",
      "2.079442    3867\n",
      "2.639057    1552\n",
      "1.791759     724\n",
      "2.708050     663\n",
      "2.995732     621\n",
      "3.044522     565\n",
      "2.197225     381\n",
      "0.000000     263\n",
      "3.367296     260\n",
      "3.526361     259\n",
      "1.945910     236\n",
      "2.397895     213\n",
      "1.098612     114\n",
      "3.091042     106\n",
      "2.772589      88\n",
      "3.555348      83\n",
      "2.302585      75\n",
      "2.833213      72\n",
      "3.663562      66\n",
      "3.496508      61\n",
      "1.609438      57\n",
      "3.258097      52\n",
      "3.401197      41\n",
      "0.693147      40\n",
      "1.386294      38\n",
      "3.178054      21\n",
      "3.688879      19\n",
      "3.761200      19\n",
      "3.737670      18\n",
      "2.484907      16\n",
      "3.784190      15\n",
      "3.218876      13\n",
      "3.295837      12\n",
      "3.806662      11\n",
      "3.465736      10\n",
      "3.637586      10\n",
      "2.890372       8\n",
      "2.944439       3\n",
      "2.564949       3\n",
      "3.713572       2\n",
      "3.828641       1\n",
      "3.135494       1\n",
      "3.433987       1\n",
      "3.583519       1\n",
      "3.332205       1\n",
      "Name: Route_2, dtype: int64\n",
      "3.218876    3491\n",
      "1.945910    3185\n",
      "1.386294    1834\n",
      "1.609438    1088\n",
      "2.197225     562\n",
      "2.302585     120\n",
      "3.135494      85\n",
      "2.079442      72\n",
      "2.639057      44\n",
      "0.000000      35\n",
      "3.178054      27\n",
      "2.484907      16\n",
      "2.772589      15\n",
      "0.693147      14\n",
      "3.401197      14\n",
      "2.708050      14\n",
      "2.397895      11\n",
      "1.098612      11\n",
      "3.258097      10\n",
      "2.564949       6\n",
      "3.091042       6\n",
      "2.833213       5\n",
      "2.890372       4\n",
      "3.433987       3\n",
      "3.367296       3\n",
      "2.995732       2\n",
      "3.332205       2\n",
      "3.044522       1\n",
      "1.791759       1\n",
      "2.944439       1\n",
      "Name: Route_3, dtype: int64\n",
      "12    9116\n",
      "5     1113\n",
      "3      302\n",
      "6       75\n",
      "9       30\n",
      "4       25\n",
      "7        8\n",
      "13       4\n",
      "10       3\n",
      "8        2\n",
      "11       1\n",
      "2        1\n",
      "1        1\n",
      "0        1\n",
      "Name: Route_4, dtype: int64\n",
      "4    10636\n",
      "1       25\n",
      "0       11\n",
      "2        7\n",
      "3        2\n",
      "5        1\n",
      "Name: Route_5, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "for i in df_train.columns:\n",
    "    print(df_train[i].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 464,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Airline            -0.680164\n",
       "Source             -0.424023\n",
       "Destination         0.283053\n",
       "Total_Stops         0.317109\n",
       "Additional_Info    -1.779689\n",
       "Price              -0.254592\n",
       "month              -0.340353\n",
       "day                -0.065171\n",
       "Arrival_Hour       -0.370146\n",
       "Arrival_Minute      0.110945\n",
       "Dep_Hour            0.112924\n",
       "Dep_Minute          0.167234\n",
       "Route_1            -0.618513\n",
       "Route_2            -0.844695\n",
       "Route_3             0.238185\n",
       "Route_4            -2.103798\n",
       "Route_5           -16.440835\n",
       "dtype: float64"
      ]
     },
     "execution_count": 464,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.skew()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.1735073  1.65835945 1.89740951 ... 1.29674414 0.40819941 0.06250046]\n",
      " [1.48441487 0.89001433 1.30041423 ... 2.14058323 3.05456196 0.06250046]\n",
      " [0.24851068 0.04055641 0.06332714 ... 0.89358023 2.28505943 0.06250046]\n",
      " ...\n",
      " [0.24851068 1.65835945 0.66032242 ... 1.29674414 0.40819941 0.06250046]\n",
      " [1.73967266 1.65835945 1.89740951 ... 1.29674414 0.40819941 0.06250046]\n",
      " [1.48441487 0.04055641 0.06332714 ... 0.89358023 2.28505943 0.06250046]]\n",
      "(array([    1,    15,    31,    49,    56,    73,   109,   125,   135,\n",
      "         155,   209,   251,   264,   336,   348,   378,   399,   402,\n",
      "         402,   432,   432,   452,   525,   530,   553,   588,   656,\n",
      "         657,   665,   667,   672,   685,   702,   712,   767,   785,\n",
      "         790,   790,   794,   822,   848,   848,   856,   856,   881,\n",
      "         893,   919,   919,   935,   958,   980,  1013,  1020,  1043,\n",
      "        1045,  1083,  1099,  1127,  1154,  1197,  1204,  1209,  1212,\n",
      "        1218,  1218,  1272,  1339,  1362,  1373,  1373,  1380,  1387,\n",
      "        1395,  1417,  1419,  1478,  1481,  1492,  1495,  1507,  1509,\n",
      "        1543,  1571,  1591,  1639,  1649,  1650,  1657,  1662,  1665,\n",
      "        1665,  1676,  1685,  1708,  1722,  1730,  1754,  1776,  1786,\n",
      "        1804,  1828,  1842,  1851,  1876,  1894,  1901,  1912,  1953,\n",
      "        1971,  1982,  1984,  1987,  2009,  2027,  2028,  2059,  2079,\n",
      "        2079,  2080,  2118,  2125,  2126,  2154,  2164,  2171,  2172,\n",
      "        2172,  2179,  2229,  2229,  2231,  2251,  2272,  2275,  2276,\n",
      "        2288,  2300,  2305,  2310,  2321,  2336,  2349,  2405,  2481,\n",
      "        2481,  2495,  2500,  2548,  2553,  2556,  2565,  2571,  2589,\n",
      "        2595,  2601,  2608,  2613,  2618,  2623,  2623,  2633,  2633,\n",
      "        2640,  2644,  2644,  2647,  2661,  2668,  2670,  2671,  2698,\n",
      "        2702,  2711,  2711,  2718,  2718,  2735,  2771,  2776,  2777,\n",
      "        2812,  2814,  2814,  2822,  2822,  2853,  2881,  2903,  2911,\n",
      "        2911,  2916,  2919,  2924,  2924,  2980,  2985,  3020,  3025,\n",
      "        3032,  3041,  3067,  3070,  3102,  3111,  3122,  3148,  3150,\n",
      "        3157,  3157,  3163,  3174,  3174,  3180,  3183,  3186,  3201,\n",
      "        3204,  3220,  3220,  3240,  3256,  3269,  3317,  3317,  3319,\n",
      "        3325,  3385,  3420,  3428,  3460,  3469,  3476,  3482,  3489,\n",
      "        3496,  3496,  3516,  3559,  3568,  3568,  3570,  3584,  3584,\n",
      "        3598,  3601,  3606,  3614,  3614,  3674,  3695,  3700,  3709,\n",
      "        3717,  3788,  3793,  3815,  3815,  3826,  3849,  3858,  3882,\n",
      "        3933,  3945,  3945,  3945,  3966,  3990,  3996,  4001,  4026,\n",
      "        4044,  4047,  4060,  4091,  4103,  4108,  4118,  4118,  4122,\n",
      "        4132,  4142,  4211,  4227,  4231,  4236,  4247,  4259,  4270,\n",
      "        4293,  4341,  4344,  4345,  4362,  4396,  4396,  4402,  4420,\n",
      "        4452,  4463,  4463,  4473,  4479,  4498,  4521,  4527,  4559,\n",
      "        4559,  4564,  4593,  4603,  4618,  4651,  4653,  4655,  4655,\n",
      "        4662,  4667,  4689,  4733,  4760,  4760,  4810,  4810,  4822,\n",
      "        4854,  4865,  4881,  4945,  4955,  4960,  4988,  4988,  5013,\n",
      "        5014,  5025,  5042,  5046,  5050,  5050,  5097,  5106,  5141,\n",
      "        5164,  5172,  5190,  5190,  5197,  5219,  5226,  5237,  5245,\n",
      "        5250,  5303,  5346,  5363,  5365,  5372,  5372,  5377,  5383,\n",
      "        5391,  5402,  5439,  5442,  5446,  5446,  5446,  5474,  5525,\n",
      "        5547,  5554,  5607,  5623,  5626,  5628,  5676,  5682,  5684,\n",
      "        5752,  5760,  5760,  5781,  5820,  5825,  5825,  5838,  5838,\n",
      "        5839,  5854,  5872,  5877,  5878,  5879,  5885,  5913,  5926,\n",
      "        5928,  5933,  5947,  5947,  5947,  5967,  5973,  5974,  5996,\n",
      "        5996,  6002,  6024,  6035,  6038,  6038,  6040,  6048,  6126,\n",
      "        6166,  6180,  6265,  6280,  6305,  6305,  6307,  6321,  6336,\n",
      "        6345,  6377,  6415,  6417,  6444,  6444,  6469,  6499,  6507,\n",
      "        6559,  6559,  6565,  6576,  6599,  6599,  6609,  6611,  6668,\n",
      "        6718,  6730,  6755,  6764,  6797,  6811,  6841,  6862,  6884,\n",
      "        6884,  6898,  6914,  6944,  6972,  6973,  6974,  6992,  6993,\n",
      "        7001,  7001,  7014,  7017,  7031,  7031,  7071,  7100,  7102,\n",
      "        7103,  7107,  7175,  7175,  7185,  7233,  7243,  7246,  7249,\n",
      "        7249,  7269,  7277,  7283,  7287,  7306,  7311,  7351,  7365,\n",
      "        7428,  7428,  7433,  7445,  7453,  7461,  7463,  7503,  7521,\n",
      "        7529,  7581,  7586,  7586,  7605,  7607,  7619,  7659,  7661,\n",
      "        7686,  7696,  7702,  7724,  7750,  7752,  7752,  7753,  7754,\n",
      "        7758,  7762,  7793,  7796,  7863,  7874,  7876,  7876,  7879,\n",
      "        7886,  7906,  7923,  7950,  7962,  7971,  7989,  8001,  8049,\n",
      "        8065,  8066,  8095,  8105,  8127,  8153,  8153,  8174,  8176,\n",
      "        8180,  8195,  8202,  8204,  8204,  8207,  8232,  8235,  8237,\n",
      "        8256,  8260,  8265,  8298,  8298,  8305,  8307,  8315,  8344,\n",
      "        8347,  8356,  8366,  8381,  8452,  8529,  8530,  8537,  8559,\n",
      "        8601,  8601,  8602,  8613,  8627,  8631,  8640,  8667,  8671,\n",
      "        8680,  8682,  8704,  8729,  8729,  8738,  8758,  8759,  8761,\n",
      "        8781,  8793,  8794,  8794,  8819,  8821,  8869,  8871,  8880,\n",
      "        8882,  8899,  8916,  8927,  8957,  8988,  9019,  9024,  9029,\n",
      "        9029,  9042,  9045,  9053,  9053,  9109,  9129,  9134,  9170,\n",
      "        9179,  9181,  9181,  9184,  9186,  9204,  9223,  9227,  9230,\n",
      "        9239,  9245,  9292,  9322,  9332,  9337,  9366,  9366,  9373,\n",
      "        9402,  9413,  9416,  9433,  9451,  9453,  9453,  9454,  9481,\n",
      "        9481,  9483,  9483,  9498,  9503,  9516,  9529,  9538,  9539,\n",
      "        9550,  9568,  9568,  9610,  9622,  9656,  9672,  9680,  9704,\n",
      "        9708,  9714,  9730,  9758,  9776,  9806,  9809,  9811,  9847,\n",
      "        9855,  9884,  9890,  9912,  9917,  9921,  9942,  9949,  9958,\n",
      "        9966,  9981, 10019, 10023, 10065, 10087, 10098, 10128, 10182,\n",
      "       10199, 10226, 10239, 10241, 10275, 10284, 10290, 10332, 10363,\n",
      "       10363, 10388, 10388, 10391, 10403, 10405, 10419, 10423, 10451,\n",
      "       10455, 10472, 10478, 10505, 10510, 10517, 10523, 10538, 10547,\n",
      "       10554, 10569, 10570, 10570, 10572, 10576, 10593, 10598, 10601,\n",
      "       10621, 10622, 10638, 10638, 10641, 10661, 10669, 10671, 10672],\n",
      "      dtype=int64), array([15, 13, 15, 13, 14, 13, 13, 15, 13, 15, 13, 13, 15, 13, 15, 15, 15,\n",
      "        3, 16, 14, 15, 13, 15, 15, 13, 13, 15,  5, 15, 13, 13, 13, 13, 13,\n",
      "       13,  4, 14, 15, 13, 13, 14, 15, 14, 15, 15, 15,  3, 16, 13, 15, 13,\n",
      "       13, 15,  4, 15, 13, 15, 15, 13, 15, 15, 15, 15,  3, 16, 13, 15, 13,\n",
      "       14, 15, 15, 13, 15, 13, 13,  5, 15, 15, 15, 13, 13, 13, 15, 15, 13,\n",
      "       13, 13, 13, 13,  3, 16, 13, 13, 13, 15, 13, 15, 13, 13, 13, 13, 15,\n",
      "       13, 15, 15, 13, 13, 15, 13, 13, 15, 13, 13, 15, 15, 15,  4, 13, 15,\n",
      "       13, 13, 13, 13, 15, 15,  3, 16, 13, 14, 15, 15, 15, 15, 13, 15, 13,\n",
      "       15, 15, 15, 14, 15, 15,  4,  4, 13,  4, 15, 13, 15,  4, 15, 15, 13,\n",
      "       13, 15, 15, 13,  5,  3, 16,  3, 16, 13, 14, 15, 15, 13, 15, 15, 15,\n",
      "       15, 15, 14, 15,  3, 16, 15, 15, 13, 15, 13,  3, 16,  3, 16, 15, 15,\n",
      "       13, 14, 15, 15, 13,  4,  5, 15, 13, 13, 13,  4, 15, 13, 13, 13,  4,\n",
      "       13, 15, 13,  3, 16, 15, 14, 15, 13, 15, 15, 13, 13,  3, 16, 13, 15,\n",
      "       13,  3, 16, 13, 15, 13, 15, 15, 15, 15, 15, 13, 13,  3, 16, 15, 13,\n",
      "        3, 16, 13,  3, 16, 15, 13, 15, 14, 15, 13, 15,  4, 13, 13,  4, 15,\n",
      "        3, 16, 13, 13, 15, 15, 13,  3, 15, 16, 13, 15, 15, 15, 13, 15, 13,\n",
      "       15, 13, 15, 15,  3, 16, 15, 13, 13, 13, 13, 15, 15, 15, 15, 15, 15,\n",
      "       13, 13, 13, 15, 14, 15, 13, 15, 13,  3, 16, 13, 13, 15,  4, 13, 14,\n",
      "       15, 15, 13, 15, 15, 15, 15,  3, 16, 15, 15, 13, 13,  3, 16, 14, 15,\n",
      "       15, 13, 13, 15, 13, 15, 15, 14, 15,  4, 15, 15, 15, 15,  3, 16, 15,\n",
      "       15, 13, 15, 15, 14, 15, 15, 15, 15, 13, 15, 13, 15, 13, 13, 13,  4,\n",
      "        5, 15, 13, 13, 13,  5, 13,  3, 15, 16, 15, 15, 15, 13, 15, 13, 13,\n",
      "        4, 13, 13, 13, 13, 14, 15, 13, 15, 14, 15,  3, 16, 15, 15, 15, 13,\n",
      "       15, 13, 15, 13, 13, 15, 13,  3, 15, 16, 13, 13, 13,  3, 16, 13,  4,\n",
      "       15, 14, 15, 15, 15, 13, 15, 15, 13, 15, 14, 15, 13, 14,  4, 13, 15,\n",
      "       13, 13,  3, 16, 13, 15, 13, 14, 15, 15,  4,  3, 16, 15, 15, 13, 15,\n",
      "       15, 15, 13, 15, 15, 15, 15,  3, 16, 13, 13, 15, 13, 15, 15, 13, 15,\n",
      "        3, 16, 15, 13,  3, 16, 13, 15, 15, 15, 15, 14, 15, 13, 13, 15, 15,\n",
      "        3, 16, 13, 13, 13, 15, 13, 13,  5, 13, 14, 15, 15, 15, 13, 13, 13,\n",
      "       15, 15, 13, 15,  3, 16, 13, 15, 15, 15, 13, 15, 15, 15,  4, 15,  3,\n",
      "       16, 15, 13, 15, 15, 13, 15, 15, 13,  3, 16, 15, 13, 15, 15, 15, 15,\n",
      "       13, 13, 15, 13, 13, 13, 15, 13, 13,  3, 16, 13, 13, 13, 15, 15,  3,\n",
      "       16, 15, 13, 15, 15, 15, 13, 15, 14, 15, 15, 15, 15, 13, 13, 13, 15,\n",
      "       13, 13, 13, 13, 13, 13,  3, 16, 13, 13, 13, 13, 13, 15, 15, 15, 13,\n",
      "       15,  3, 16, 15, 15, 15, 15, 15, 15, 14, 15, 13, 13, 15, 15, 15, 13,\n",
      "       15, 13, 13,  4, 15,  4, 13,  3, 16, 15, 15, 14, 15, 13, 13, 15, 13,\n",
      "       15,  3, 16, 15, 15, 15, 13, 15, 15, 15, 13, 15, 13, 13, 13, 14, 15,\n",
      "       13, 13, 15, 13, 13, 15,  3, 16, 13, 14, 15,  3, 16, 15, 15, 15, 13,\n",
      "       13, 13, 13,  3, 16, 13, 13,  4, 13, 15, 15, 15,  5, 13, 13, 15, 15,\n",
      "       13, 15, 14, 13, 15, 13, 15, 13, 13, 15, 15, 13, 13, 15, 13, 15, 13,\n",
      "       15, 13, 13, 15, 15, 15, 15, 13, 15, 15, 13, 15,  4,  5, 14, 15, 13,\n",
      "       15, 13, 13, 14, 13, 13, 15, 15, 13,  4, 15, 13, 15, 13, 15, 15, 14,\n",
      "       15, 13, 15, 13, 13, 13, 15, 13,  3, 16, 15, 15, 15, 13, 13],\n",
      "      dtype=int64))\n"
     ]
    }
   ],
   "source": [
    "#removing outliers using zscore\n",
    "from scipy.stats import zscore\n",
    "zscore=np.abs(zscore(df_train))\n",
    "print(zscore)\n",
    "print(np.where(zscore>3))\n",
    "df_train_new=df_train[(zscore<3).all(axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before removing outliers: (10682, 17)\n",
      "After removing outliers: (10037, 17)\n"
     ]
    }
   ],
   "source": [
    "print('Before removing outliers:',df_train.shape)\n",
    "print('After removing outliers:',df_train_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 467,
   "metadata": {},
   "outputs": [],
   "source": [
    "#separating into input and output variables\n",
    "df_x=df_train_new.drop(columns=['Price'])\n",
    "y=pd.DataFrame(df_train_new['Price'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 468,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "sc=StandardScaler()\n",
    "x=sc.fit_transform(df_x)\n",
    "x=pd.DataFrame(x,columns=df_x.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 469,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10037, 16)"
      ]
     },
     "execution_count": 469,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10037, 1)"
      ]
     },
     "execution_count": 470,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_absolute_error,mean_squared_error,r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "def maxr2_score(regr,x,y):\n",
    "    max_r_score=0\n",
    "    for r_state in range(42,100):\n",
    "        x_train,x_test,y_train,y_test=train_test_split(x,y,random_state=r_state,test_size=0.20)\n",
    "        regr.fit(x_train,y_train)\n",
    "        y_pred=regr.predict(x_test)\n",
    "        r2_scr=r2_score(y_test,y_pred)\n",
    "        print(\"r2 score corresponding to\",r_state,\"is\",r2_scr)\n",
    "        if r2_scr>max_r_score:\n",
    "            max_r_score=r2_scr\n",
    "            final_r_state=r_state\n",
    "    print(\"max r2 score corresponding to\",final_r_state,\"is\",max_r_score)    \n",
    "    return final_r_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 472,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2 score corresponding to 42 is 0.6263302030570931\n",
      "r2 score corresponding to 43 is 0.6077003496248179\n",
      "r2 score corresponding to 44 is 0.6283337568448917\n",
      "r2 score corresponding to 45 is 0.6122563244303569\n",
      "r2 score corresponding to 46 is 0.6164636635152704\n",
      "r2 score corresponding to 47 is 0.624172529794397\n",
      "r2 score corresponding to 48 is 0.6191223309789333\n",
      "r2 score corresponding to 49 is 0.6104193998511522\n",
      "r2 score corresponding to 50 is 0.624592881021135\n",
      "r2 score corresponding to 51 is 0.5930201819674198\n",
      "r2 score corresponding to 52 is 0.6289619460396887\n",
      "r2 score corresponding to 53 is 0.599192586664314\n",
      "r2 score corresponding to 54 is 0.6186265769607864\n",
      "r2 score corresponding to 55 is 0.6187764995610041\n",
      "r2 score corresponding to 56 is 0.6221621328353419\n",
      "r2 score corresponding to 57 is 0.6237628511561364\n",
      "r2 score corresponding to 58 is 0.6164244498630461\n",
      "r2 score corresponding to 59 is 0.6163435271366037\n",
      "r2 score corresponding to 60 is 0.6127914379087509\n",
      "r2 score corresponding to 61 is 0.6100464258484033\n",
      "r2 score corresponding to 62 is 0.616817349915032\n",
      "r2 score corresponding to 63 is 0.6272840349313689\n",
      "r2 score corresponding to 64 is 0.6015963748557935\n",
      "r2 score corresponding to 65 is 0.6260272328845357\n",
      "r2 score corresponding to 66 is 0.6122166320321218\n",
      "r2 score corresponding to 67 is 0.6159948831981634\n",
      "r2 score corresponding to 68 is 0.6114322053480018\n",
      "r2 score corresponding to 69 is 0.615693593777115\n",
      "r2 score corresponding to 70 is 0.6022472280182598\n",
      "r2 score corresponding to 71 is 0.6183849135806919\n",
      "r2 score corresponding to 72 is 0.6044026572578503\n",
      "r2 score corresponding to 73 is 0.5777825398547063\n",
      "r2 score corresponding to 74 is 0.6135099553461141\n",
      "r2 score corresponding to 75 is 0.6293074293218788\n",
      "r2 score corresponding to 76 is 0.6074372367342813\n",
      "r2 score corresponding to 77 is 0.6115610466850281\n",
      "r2 score corresponding to 78 is 0.615493895846996\n",
      "r2 score corresponding to 79 is 0.6222906505595536\n",
      "r2 score corresponding to 80 is 0.6102733851948754\n",
      "r2 score corresponding to 81 is 0.6151414684221255\n",
      "r2 score corresponding to 82 is 0.6167365192433946\n",
      "r2 score corresponding to 83 is 0.595955643169932\n",
      "r2 score corresponding to 84 is 0.6162907478347139\n",
      "r2 score corresponding to 85 is 0.623257516413616\n",
      "r2 score corresponding to 86 is 0.5995023157481205\n",
      "r2 score corresponding to 87 is 0.602586652173295\n",
      "r2 score corresponding to 88 is 0.6033738079921547\n",
      "r2 score corresponding to 89 is 0.6170787032075837\n",
      "r2 score corresponding to 90 is 0.593609931173992\n",
      "r2 score corresponding to 91 is 0.5950984930975876\n",
      "r2 score corresponding to 92 is 0.6062179042412139\n",
      "r2 score corresponding to 93 is 0.6155452417880468\n",
      "r2 score corresponding to 94 is 0.6069372409178435\n",
      "r2 score corresponding to 95 is 0.6080035515393457\n",
      "r2 score corresponding to 96 is 0.5933757880051852\n",
      "r2 score corresponding to 97 is 0.595943687247454\n",
      "r2 score corresponding to 98 is 0.6076517676290834\n",
      "r2 score corresponding to 99 is 0.6206826844688099\n",
      "max r2 score corresponding to 75 is 0.6293074293218788\n"
     ]
    }
   ],
   "source": [
    "#Using LinearRegression and checking maxr2 score corresponding to different random states\n",
    "from sklearn.linear_model import LinearRegression\n",
    "lreg=LinearRegression()\n",
    "r_state=maxr2_score(lreg,x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 473,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'n_neighbors': 5}"
      ]
     },
     "execution_count": 473,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "neighbors={'n_neighbors':range(1,26)}\n",
    "knr=KNeighborsRegressor()\n",
    "gknr=GridSearchCV(knr,neighbors,cv=10)\n",
    "gknr.fit(x,y)\n",
    "gknr.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 474,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2 score corresponding to 42 is 0.8161660274808328\n",
      "r2 score corresponding to 43 is 0.8034907552965579\n",
      "r2 score corresponding to 44 is 0.7969917771046644\n",
      "r2 score corresponding to 45 is 0.8164717970001218\n",
      "r2 score corresponding to 46 is 0.8046322376016389\n",
      "r2 score corresponding to 47 is 0.8104355614902791\n",
      "r2 score corresponding to 48 is 0.7877821366627455\n",
      "r2 score corresponding to 49 is 0.8019996468435892\n",
      "r2 score corresponding to 50 is 0.8157655025716819\n",
      "r2 score corresponding to 51 is 0.7950084293867594\n",
      "r2 score corresponding to 52 is 0.829668147352094\n",
      "r2 score corresponding to 53 is 0.789235601812571\n",
      "r2 score corresponding to 54 is 0.809373102809452\n",
      "r2 score corresponding to 55 is 0.801391156442027\n",
      "r2 score corresponding to 56 is 0.7911832966862453\n",
      "r2 score corresponding to 57 is 0.8036994529552888\n",
      "r2 score corresponding to 58 is 0.8143856906562905\n",
      "r2 score corresponding to 59 is 0.8080829466285147\n",
      "r2 score corresponding to 60 is 0.7931921723844361\n",
      "r2 score corresponding to 61 is 0.7955688215813982\n",
      "r2 score corresponding to 62 is 0.797491307404705\n",
      "r2 score corresponding to 63 is 0.8109503538419899\n",
      "r2 score corresponding to 64 is 0.8119976384465386\n",
      "r2 score corresponding to 65 is 0.8085664726753286\n",
      "r2 score corresponding to 66 is 0.8070874868355429\n",
      "r2 score corresponding to 67 is 0.7997814052475081\n",
      "r2 score corresponding to 68 is 0.8088566950185319\n",
      "r2 score corresponding to 69 is 0.8103874877060646\n",
      "r2 score corresponding to 70 is 0.7839299601289785\n",
      "r2 score corresponding to 71 is 0.7949524390156726\n",
      "r2 score corresponding to 72 is 0.7861498033246763\n",
      "r2 score corresponding to 73 is 0.7795460602203558\n",
      "r2 score corresponding to 74 is 0.8128989758920155\n",
      "r2 score corresponding to 75 is 0.8233583322442455\n",
      "r2 score corresponding to 76 is 0.7949319283828703\n",
      "r2 score corresponding to 77 is 0.8107729118460897\n",
      "r2 score corresponding to 78 is 0.811941324714744\n",
      "r2 score corresponding to 79 is 0.8018811568236045\n",
      "r2 score corresponding to 80 is 0.7926339950992582\n",
      "r2 score corresponding to 81 is 0.7876478019889688\n",
      "r2 score corresponding to 82 is 0.8038434283956268\n",
      "r2 score corresponding to 83 is 0.7883826011593902\n",
      "r2 score corresponding to 84 is 0.8123013625521347\n",
      "r2 score corresponding to 85 is 0.8102461723685498\n",
      "r2 score corresponding to 86 is 0.7913930455319447\n",
      "r2 score corresponding to 87 is 0.7747884535169081\n",
      "r2 score corresponding to 88 is 0.7977421430282681\n",
      "r2 score corresponding to 89 is 0.8133444116660453\n",
      "r2 score corresponding to 90 is 0.7774419386735444\n",
      "r2 score corresponding to 91 is 0.7667322292201583\n",
      "r2 score corresponding to 92 is 0.8022526451164679\n",
      "r2 score corresponding to 93 is 0.8136203081069178\n",
      "r2 score corresponding to 94 is 0.7803279226136377\n",
      "r2 score corresponding to 95 is 0.8015927995228406\n",
      "r2 score corresponding to 96 is 0.787040314338727\n",
      "r2 score corresponding to 97 is 0.785014269708632\n",
      "r2 score corresponding to 98 is 0.7861625276929985\n",
      "r2 score corresponding to 99 is 0.8068731445255686\n",
      "max r2 score corresponding to 52 is 0.829668147352094\n"
     ]
    }
   ],
   "source": [
    "#using KNN regression and checking max r2 score corresponding to different random states\n",
    "knr=KNeighborsRegressor(n_neighbors=5)\n",
    "r_state=maxr2_score(knr,x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 476,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean r2 score for Linear Regression: 0.6093230963949587\n",
      "Standard Deviation in r2 score for Linear Regression: 0.016417755913385637\n",
      "\n",
      "Mean r2 score for KNN Regression: 0.7997421021966302\n",
      "Standard Deviation in r2 score for KNN Regression: 0.009072624215766349\n"
     ]
    }
   ],
   "source": [
    "#checking the mean r2 score of both Linear Regression Model and KNN Regression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "print('Mean r2 score for Linear Regression:',cross_val_score(lreg,x,y,cv=5,scoring='r2').mean())\n",
    "print('Standard Deviation in r2 score for Linear Regression:',cross_val_score(lreg,x,y,cv=5,scoring='r2').std())\n",
    "print()\n",
    "print(\"Mean r2 score for KNN Regression:\",cross_val_score(knr,x,y,cv=5,scoring='r2').mean())\n",
    "print('Standard Deviation in r2 score for KNN Regression:',cross_val_score(knr,x,y,cv=5,scoring='r2').std())\n",
    "#Based on below output KNN Regression  is performing well"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 0.0001}"
      ]
     },
     "execution_count": 477,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking lasso Regression and finding best value for alpha\n",
    "from sklearn.linear_model import Lasso\n",
    "lsreg=Lasso()\n",
    "parameters={\"alpha\":[0.0001,0.001,0.01,0.1,1]}\n",
    "clf=GridSearchCV(lsreg,parameters,cv=10)\n",
    "clf.fit(x,y)\n",
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2 score corresponding to 42 is 0.6262862246976229\n",
      "r2 score corresponding to 43 is 0.6076922064883095\n",
      "r2 score corresponding to 44 is 0.6282852228787021\n",
      "r2 score corresponding to 45 is 0.6122581567231993\n",
      "r2 score corresponding to 46 is 0.6164394838862882\n",
      "r2 score corresponding to 47 is 0.6242129644458152\n",
      "r2 score corresponding to 48 is 0.6190880063400409\n",
      "r2 score corresponding to 49 is 0.6103748404829656\n",
      "r2 score corresponding to 50 is 0.6246124332246051\n",
      "r2 score corresponding to 51 is 0.5930270895325418\n",
      "r2 score corresponding to 52 is 0.6290002799270912\n",
      "r2 score corresponding to 53 is 0.5992260878963571\n",
      "r2 score corresponding to 54 is 0.6186117198474762\n",
      "r2 score corresponding to 55 is 0.6187263152087117\n",
      "r2 score corresponding to 56 is 0.6221159878304593\n",
      "r2 score corresponding to 57 is 0.6236843564174066\n",
      "r2 score corresponding to 58 is 0.6164452596809273\n",
      "r2 score corresponding to 59 is 0.6163198693308121\n",
      "r2 score corresponding to 60 is 0.6128355462888276\n",
      "r2 score corresponding to 61 is 0.6099978643032762\n",
      "r2 score corresponding to 62 is 0.6167901946077208\n",
      "r2 score corresponding to 63 is 0.6272848358744145\n",
      "r2 score corresponding to 64 is 0.6015744683146484\n",
      "r2 score corresponding to 65 is 0.6259802125312909\n",
      "r2 score corresponding to 66 is 0.6122188128087893\n",
      "r2 score corresponding to 67 is 0.6159710093015963\n",
      "r2 score corresponding to 68 is 0.61136373814852\n",
      "r2 score corresponding to 69 is 0.6156622169206833\n",
      "r2 score corresponding to 70 is 0.602271642922102\n",
      "r2 score corresponding to 71 is 0.6183780336346459\n",
      "r2 score corresponding to 72 is 0.6044167560740807\n",
      "r2 score corresponding to 73 is 0.5777695868194304\n",
      "r2 score corresponding to 74 is 0.6134593710792099\n",
      "r2 score corresponding to 75 is 0.629322132842943\n",
      "r2 score corresponding to 76 is 0.6074484371152347\n",
      "r2 score corresponding to 77 is 0.6115396479634847\n",
      "r2 score corresponding to 78 is 0.6155045767382499\n",
      "r2 score corresponding to 79 is 0.6223314859571575\n",
      "r2 score corresponding to 80 is 0.6102619025007542\n",
      "r2 score corresponding to 81 is 0.6151666302381171\n",
      "r2 score corresponding to 82 is 0.6167398342481651\n",
      "r2 score corresponding to 83 is 0.5959697343079798\n",
      "r2 score corresponding to 84 is 0.6163088008404085\n",
      "r2 score corresponding to 85 is 0.6232290246464167\n",
      "r2 score corresponding to 86 is 0.59952999358222\n",
      "r2 score corresponding to 87 is 0.6026066113840352\n",
      "r2 score corresponding to 88 is 0.6033223398259502\n",
      "r2 score corresponding to 89 is 0.6170854816412678\n",
      "r2 score corresponding to 90 is 0.5936150067271385\n",
      "r2 score corresponding to 91 is 0.5950659559589755\n",
      "r2 score corresponding to 92 is 0.6062145625088213\n",
      "r2 score corresponding to 93 is 0.6155581108852836\n",
      "r2 score corresponding to 94 is 0.6069293586155893\n",
      "r2 score corresponding to 95 is 0.6080101978975017\n",
      "r2 score corresponding to 96 is 0.593414108450357\n",
      "r2 score corresponding to 97 is 0.5960304826610918\n",
      "r2 score corresponding to 98 is 0.6076589440714494\n",
      "r2 score corresponding to 99 is 0.6206440487490785\n",
      "max r2 score corresponding to 75 is 0.629322132842943\n"
     ]
    }
   ],
   "source": [
    "#Checking max r2 score when we use Lasso\n",
    "lsreg=Lasso(alpha=0.0001)\n",
    "r_state=maxr2_score(lsreg,x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 482,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean r2 score for Lasso Regression: 0.6093234595899599\n",
      "Standard Deviation in r2 score for Lasso Regression: 0.016398224907715742\n"
     ]
    }
   ],
   "source": [
    "#Using cross val score with lasso\n",
    "print(\"Mean r2 score for Lasso Regression:\",cross_val_score(lsreg,x,y,cv=5,scoring='r2').mean())\n",
    "print('Standard Deviation in r2 score for Lasso Regression:',cross_val_score(lsreg,x,y,cv=5,scoring='r2').std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'alpha': 1}"
      ]
     },
     "execution_count": 480,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Checking Ridge regression and finding best value for alpha\n",
    "from sklearn.linear_model import Ridge\n",
    "rdreg=Ridge()\n",
    "parameters={\"alpha\":[0.001,0.01,0.1,1]}\n",
    "clf=GridSearchCV(rdreg,parameters,cv=10)\n",
    "clf.fit(x,y)\n",
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2 score corresponding to 42 is 0.6263206573877469\n",
      "r2 score corresponding to 43 is 0.6076975550504764\n",
      "r2 score corresponding to 44 is 0.6283275898477776\n",
      "r2 score corresponding to 45 is 0.6122560650467775\n",
      "r2 score corresponding to 46 is 0.6164591840263829\n",
      "r2 score corresponding to 47 is 0.624190197866681\n",
      "r2 score corresponding to 48 is 0.6191048189271808\n",
      "r2 score corresponding to 49 is 0.6104071283731499\n",
      "r2 score corresponding to 50 is 0.6245940688510273\n",
      "r2 score corresponding to 51 is 0.5930235043085929\n",
      "r2 score corresponding to 52 is 0.6289587875789959\n",
      "r2 score corresponding to 53 is 0.5991993105723843\n",
      "r2 score corresponding to 54 is 0.6186185974473877\n",
      "r2 score corresponding to 55 is 0.6187654879515287\n",
      "r2 score corresponding to 56 is 0.6221467132184898\n",
      "r2 score corresponding to 57 is 0.6237472083045996\n",
      "r2 score corresponding to 58 is 0.6164184287703902\n",
      "r2 score corresponding to 59 is 0.616325699155738\n",
      "r2 score corresponding to 60 is 0.6127872051077137\n",
      "r2 score corresponding to 61 is 0.6100301306575923\n",
      "r2 score corresponding to 62 is 0.6168128774208448\n",
      "r2 score corresponding to 63 is 0.6272827945126125\n",
      "r2 score corresponding to 64 is 0.6015902070576788\n",
      "r2 score corresponding to 65 is 0.626024962483814\n",
      "r2 score corresponding to 66 is 0.6122092207842039\n",
      "r2 score corresponding to 67 is 0.6159921950566265\n",
      "r2 score corresponding to 68 is 0.6114158551444882\n",
      "r2 score corresponding to 69 is 0.6156882171164317\n",
      "r2 score corresponding to 70 is 0.6022529264160655\n",
      "r2 score corresponding to 71 is 0.6183875832697746\n",
      "r2 score corresponding to 72 is 0.604402245114086\n",
      "r2 score corresponding to 73 is 0.5777897658965006\n",
      "r2 score corresponding to 74 is 0.6134898156568207\n",
      "r2 score corresponding to 75 is 0.6293072812584495\n",
      "r2 score corresponding to 76 is 0.607454713706139\n",
      "r2 score corresponding to 77 is 0.6115560724731773\n",
      "r2 score corresponding to 78 is 0.6155072161332535\n",
      "r2 score corresponding to 79 is 0.6222996567234312\n",
      "r2 score corresponding to 80 is 0.6102753860328818\n",
      "r2 score corresponding to 81 is 0.6151391531777979\n",
      "r2 score corresponding to 82 is 0.6167396583777356\n",
      "r2 score corresponding to 83 is 0.5959580818973282\n",
      "r2 score corresponding to 84 is 0.6162946316735771\n",
      "r2 score corresponding to 85 is 0.6232472885202762\n",
      "r2 score corresponding to 86 is 0.5995096560159432\n",
      "r2 score corresponding to 87 is 0.6025919411681724\n",
      "r2 score corresponding to 88 is 0.6033618967145045\n",
      "r2 score corresponding to 89 is 0.6170744313828099\n",
      "r2 score corresponding to 90 is 0.5936205724798946\n",
      "r2 score corresponding to 91 is 0.5950934458030936\n",
      "r2 score corresponding to 92 is 0.6062213566031218\n",
      "r2 score corresponding to 93 is 0.6155341746382894\n",
      "r2 score corresponding to 94 is 0.6069409559571669\n",
      "r2 score corresponding to 95 is 0.6080095476501146\n",
      "r2 score corresponding to 96 is 0.5933930933722332\n",
      "r2 score corresponding to 97 is 0.5959745657099351\n",
      "r2 score corresponding to 98 is 0.6076571888886304\n",
      "r2 score corresponding to 99 is 0.6206761838156833\n",
      "max r2 score corresponding to 75 is 0.6293072812584495\n"
     ]
    }
   ],
   "source": [
    "#Checking max r2 score when we use Ridge\n",
    "rdreg=Ridge(alpha=1)\n",
    "r_state=maxr2_score(rdreg,x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 485,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean r2 score for Ridge Regression: 0.6093236636685468\n",
      "Standard Deviation in r2 score for Ridge Regression: 0.016412137935453663\n"
     ]
    }
   ],
   "source": [
    "#Using cross val score with Ridge\n",
    "print(\"Mean r2 score for Ridge Regression:\",cross_val_score(rdreg,x,y,cv=5,scoring='r2').mean())\n",
    "print('Standard Deviation in r2 score for Ridge Regression:',cross_val_score(rdreg,x,y,cv=5,scoring='r2').std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 487,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 1, 'n_estimators': 500}"
      ]
     },
     "execution_count": 487,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Trying to use Gradient Boosting Technique\n",
    "#For getting best set of parameters, using GridSearchCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "gbr=GradientBoostingRegressor()\n",
    "parameters={'learning_rate':[0.0001,0.001,0.01,0.1,1],'n_estimators':[50,100,150,200,250,300,350,400,450,500]}\n",
    "clf=GridSearchCV(gbr,parameters,cv=5)\n",
    "clf.fit(x,y)\n",
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 488,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean r2 score for Gradient Boosting Regression: 0.9107666863492335\n",
      "Standard Deviation in r2 score for Gradient Boosting Regression: 0.006730897605074264\n"
     ]
    }
   ],
   "source": [
    "#Using cross val score to check the mean r2 score and standard deviation\n",
    "gbr=GradientBoostingRegressor(learning_rate=1,n_estimators=500)\n",
    "print(\"Mean r2 score for Gradient Boosting Regression:\",cross_val_score(gbr,x,y,cv=5,scoring='r2').mean())\n",
    "print('Standard Deviation in r2 score for Gradient Boosting Regression:',cross_val_score(gbr,x,y,cv=5,scoring='r2').std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2 score corresponding to 42 is 0.9222839158148187\n",
      "r2 score corresponding to 43 is 0.9139830281999682\n",
      "r2 score corresponding to 44 is 0.9176576256298077\n",
      "r2 score corresponding to 45 is 0.909502967123524\n",
      "r2 score corresponding to 46 is 0.9069234422317077\n",
      "r2 score corresponding to 47 is 0.9104548882923327\n",
      "r2 score corresponding to 48 is 0.9022445330663411\n",
      "r2 score corresponding to 49 is 0.9212664638135807\n",
      "r2 score corresponding to 50 is 0.9170723350400648\n",
      "r2 score corresponding to 51 is 0.9060223558968195\n",
      "r2 score corresponding to 52 is 0.9238750818641713\n",
      "r2 score corresponding to 53 is 0.9011472949793311\n",
      "r2 score corresponding to 54 is 0.9129498510418621\n",
      "r2 score corresponding to 55 is 0.9106000516082468\n",
      "r2 score corresponding to 56 is 0.9127176893717263\n",
      "r2 score corresponding to 57 is 0.9151974775337391\n",
      "r2 score corresponding to 58 is 0.91838728884607\n",
      "r2 score corresponding to 59 is 0.9105582723583691\n",
      "r2 score corresponding to 60 is 0.9147401808543921\n",
      "r2 score corresponding to 61 is 0.9147771627835208\n",
      "r2 score corresponding to 62 is 0.9077621776706315\n",
      "r2 score corresponding to 63 is 0.9215613029069645\n",
      "r2 score corresponding to 64 is 0.9154720406025468\n",
      "r2 score corresponding to 65 is 0.9120189248825984\n",
      "r2 score corresponding to 66 is 0.912912993651666\n",
      "r2 score corresponding to 67 is 0.9139389268808332\n",
      "r2 score corresponding to 68 is 0.9111278724767582\n",
      "r2 score corresponding to 69 is 0.9131434862984574\n",
      "r2 score corresponding to 70 is 0.9023945042941628\n",
      "r2 score corresponding to 71 is 0.9034552288551763\n",
      "r2 score corresponding to 72 is 0.9128033780126761\n",
      "r2 score corresponding to 73 is 0.903917557160985\n",
      "r2 score corresponding to 74 is 0.9225038970578887\n",
      "r2 score corresponding to 75 is 0.9208694890948442\n",
      "r2 score corresponding to 76 is 0.9046564588085786\n",
      "r2 score corresponding to 77 is 0.9111465150192682\n",
      "r2 score corresponding to 78 is 0.9219094936849757\n",
      "r2 score corresponding to 79 is 0.913541617259787\n",
      "r2 score corresponding to 80 is 0.9118377172504784\n",
      "r2 score corresponding to 81 is 0.9072472209558364\n",
      "r2 score corresponding to 82 is 0.896398398742708\n",
      "r2 score corresponding to 83 is 0.9113023689183115\n",
      "r2 score corresponding to 84 is 0.9161149025334224\n",
      "r2 score corresponding to 85 is 0.9146619662485528\n",
      "r2 score corresponding to 86 is 0.9177400053026639\n",
      "r2 score corresponding to 87 is 0.9053764525479673\n",
      "r2 score corresponding to 88 is 0.9177950636544574\n",
      "r2 score corresponding to 89 is 0.917043821748229\n",
      "r2 score corresponding to 90 is 0.9068789493643856\n",
      "r2 score corresponding to 91 is 0.9038603085857769\n",
      "r2 score corresponding to 92 is 0.9194386440310797\n",
      "r2 score corresponding to 93 is 0.9154263549551014\n",
      "r2 score corresponding to 94 is 0.9080082780413627\n",
      "r2 score corresponding to 95 is 0.9182761877845879\n",
      "r2 score corresponding to 96 is 0.9035741304910335\n",
      "r2 score corresponding to 97 is 0.905507948101806\n",
      "r2 score corresponding to 98 is 0.9093328143815224\n",
      "r2 score corresponding to 99 is 0.9178994448379929\n",
      "max r2 score corresponding to 52 is 0.9238750818641713\n"
     ]
    }
   ],
   "source": [
    "#checking maximum r2 score corresponding to GradientBoostingRegressor\n",
    "r_state=maxr2_score(gbr,x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'base_estimator': DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=None,\n",
       "                       max_features=None, max_leaf_nodes=None,\n",
       "                       min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "                       min_samples_leaf=1, min_samples_split=2,\n",
       "                       min_weight_fraction_leaf=0.0, presort='deprecated',\n",
       "                       random_state=None, splitter='best'),\n",
       " 'learning_rate': 0.1,\n",
       " 'n_estimators': 500}"
      ]
     },
     "execution_count": 490,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Using Ada Boost regression algorithm\n",
    "from sklearn.ensemble import AdaBoostRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "ada_reg=AdaBoostRegressor()\n",
    "parameters={'learning_rate':[0.0001,0.001,0.01,0.1,1],'n_estimators':[50,100,150,200,250,300,350,400,450,500],'base_estimator':[lreg,lsreg,DecisionTreeRegressor()]}\n",
    "clf=GridSearchCV(ada_reg,parameters,cv=5)\n",
    "clf.fit(x,y)\n",
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean r2 score for Ada Boost Regression: 0.8879015461717549\n",
      "Standard Deviation in r2 score for Ada Boost Regression: 0.009571540694478372\n"
     ]
    }
   ],
   "source": [
    "ada_reg=AdaBoostRegressor(base_estimator=DecisionTreeRegressor(),learning_rate=0.1,n_estimators=500)\n",
    "print(\"Mean r2 score for Ada Boost Regression:\",cross_val_score(ada_reg,x,y,cv=5,scoring='r2').mean())\n",
    "print('Standard Deviation in r2 score for Ada Boost Regression:',cross_val_score(ada_reg,x,y,cv=5,scoring='r2').std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 492,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2 score corresponding to 42 is 0.8877978258209408\n",
      "r2 score corresponding to 43 is 0.8843423111119184\n",
      "r2 score corresponding to 44 is 0.8974422638935768\n",
      "r2 score corresponding to 45 is 0.8985180984730269\n",
      "r2 score corresponding to 46 is 0.8873726914764877\n",
      "r2 score corresponding to 47 is 0.8924787523305666\n",
      "r2 score corresponding to 48 is 0.8791340640295535\n",
      "r2 score corresponding to 49 is 0.8910414172412445\n",
      "r2 score corresponding to 50 is 0.887627630949987\n",
      "r2 score corresponding to 51 is 0.8679854943953508\n",
      "r2 score corresponding to 52 is 0.9050695288440931\n",
      "r2 score corresponding to 53 is 0.8761726565754783\n",
      "r2 score corresponding to 54 is 0.8844538182130594\n",
      "r2 score corresponding to 55 is 0.8853304913693181\n",
      "r2 score corresponding to 56 is 0.887853922309132\n",
      "r2 score corresponding to 57 is 0.876931890627588\n",
      "r2 score corresponding to 58 is 0.8863591108691788\n",
      "r2 score corresponding to 59 is 0.8770450619521801\n",
      "r2 score corresponding to 60 is 0.8844123894749324\n",
      "r2 score corresponding to 61 is 0.8881482847238398\n",
      "r2 score corresponding to 62 is 0.8800242431848652\n",
      "r2 score corresponding to 63 is 0.8962278692931336\n",
      "r2 score corresponding to 64 is 0.8837770628515372\n",
      "r2 score corresponding to 65 is 0.8865782900899604\n",
      "r2 score corresponding to 66 is 0.8879299112454682\n",
      "r2 score corresponding to 67 is 0.8821584162528291\n",
      "r2 score corresponding to 68 is 0.8910062641750076\n",
      "r2 score corresponding to 69 is 0.8910805141392777\n",
      "r2 score corresponding to 70 is 0.8827259253170683\n",
      "r2 score corresponding to 71 is 0.8742016294874537\n",
      "r2 score corresponding to 72 is 0.8811896740459183\n",
      "r2 score corresponding to 73 is 0.8796661398967477\n",
      "r2 score corresponding to 74 is 0.9044726361675732\n",
      "r2 score corresponding to 75 is 0.896258844883713\n",
      "r2 score corresponding to 76 is 0.8822683958086238\n",
      "r2 score corresponding to 77 is 0.8870924135621455\n",
      "r2 score corresponding to 78 is 0.8876506146217976\n",
      "r2 score corresponding to 79 is 0.8915497954703495\n",
      "r2 score corresponding to 80 is 0.8852448225170889\n",
      "r2 score corresponding to 81 is 0.8885761031051144\n",
      "r2 score corresponding to 82 is 0.8875103444941673\n",
      "r2 score corresponding to 83 is 0.8895139533615511\n",
      "r2 score corresponding to 84 is 0.8985878512782582\n",
      "r2 score corresponding to 85 is 0.8762547090922598\n",
      "r2 score corresponding to 86 is 0.8729678493476937\n",
      "r2 score corresponding to 87 is 0.8755106125849061\n",
      "r2 score corresponding to 88 is 0.8836794934406096\n",
      "r2 score corresponding to 89 is 0.8944898722602481\n",
      "r2 score corresponding to 90 is 0.8671696553307999\n",
      "r2 score corresponding to 91 is 0.8763607036421421\n",
      "r2 score corresponding to 92 is 0.8947512390992456\n",
      "r2 score corresponding to 93 is 0.9006951382485001\n",
      "r2 score corresponding to 94 is 0.8759562378692277\n",
      "r2 score corresponding to 95 is 0.888234546408736\n",
      "r2 score corresponding to 96 is 0.887546473744194\n",
      "r2 score corresponding to 97 is 0.879699368200567\n",
      "r2 score corresponding to 98 is 0.8810900095636535\n",
      "r2 score corresponding to 99 is 0.8956340802317062\n",
      "max r2 score corresponding to 52 is 0.9050695288440931\n"
     ]
    }
   ],
   "source": [
    "#checking maximum r2 score corresponding to Ada Boost\n",
    "r_state=maxr2_score(ada_reg,x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 495,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "r2 score corresponding to 42 is 0.6212316209888924\n",
      "r2 score corresponding to 43 is 0.604911936830849\n",
      "r2 score corresponding to 44 is 0.6232969777920967\n",
      "r2 score corresponding to 45 is 0.6093248041766927\n",
      "r2 score corresponding to 46 is 0.6126463530461943\n",
      "r2 score corresponding to 47 is 0.6242596415965764\n",
      "r2 score corresponding to 48 is 0.6143788946388218\n",
      "r2 score corresponding to 49 is 0.6054933422529991\n",
      "r2 score corresponding to 50 is 0.6231323990438371\n",
      "r2 score corresponding to 51 is 0.5910623315494138\n",
      "r2 score corresponding to 52 is 0.6278766975761043\n",
      "r2 score corresponding to 53 is 0.5979703231057867\n",
      "r2 score corresponding to 54 is 0.6149891768822616\n",
      "r2 score corresponding to 55 is 0.6132517646926221\n",
      "r2 score corresponding to 56 is 0.6167814032037281\n",
      "r2 score corresponding to 57 is 0.6167808010095513\n",
      "r2 score corresponding to 58 is 0.6145451455008286\n",
      "r2 score corresponding to 59 is 0.6119012999554251\n",
      "r2 score corresponding to 60 is 0.61177146605866\n",
      "r2 score corresponding to 61 is 0.6048380328011733\n",
      "r2 score corresponding to 62 is 0.6124435025140726\n",
      "r2 score corresponding to 63 is 0.6248761375942884\n",
      "r2 score corresponding to 64 is 0.5979773928014942\n",
      "r2 score corresponding to 65 is 0.6221876354544528\n",
      "r2 score corresponding to 66 is 0.609400702447719\n",
      "r2 score corresponding to 67 is 0.6121886295526262\n",
      "r2 score corresponding to 68 is 0.6049421943840179\n",
      "r2 score corresponding to 69 is 0.6111523271555295\n",
      "r2 score corresponding to 70 is 0.6011626864274143\n",
      "r2 score corresponding to 71 is 0.6155061059600548\n",
      "r2 score corresponding to 72 is 0.6024408379652271\n",
      "r2 score corresponding to 73 is 0.5753436601311035\n",
      "r2 score corresponding to 74 is 0.6078090920464893\n",
      "r2 score corresponding to 75 is 0.6275693638137689\n",
      "r2 score corresponding to 76 is 0.606060862803631\n",
      "r2 score corresponding to 77 is 0.608038126472767\n",
      "r2 score corresponding to 78 is 0.6136817888328616\n",
      "r2 score corresponding to 79 is 0.6218599050770418\n",
      "r2 score corresponding to 80 is 0.6070395826623265\n",
      "r2 score corresponding to 81 is 0.6136598147217194\n",
      "r2 score corresponding to 82 is 0.6144257382924455\n",
      "r2 score corresponding to 83 is 0.5944587685578376\n",
      "r2 score corresponding to 84 is 0.6144892777704636\n",
      "r2 score corresponding to 85 is 0.6186236548710567\n",
      "r2 score corresponding to 86 is 0.5981082950115401\n",
      "r2 score corresponding to 87 is 0.6008384663560868\n",
      "r2 score corresponding to 88 is 0.5976454699651047\n",
      "r2 score corresponding to 89 is 0.6146778382968741\n",
      "r2 score corresponding to 90 is 0.5913166136010191\n",
      "r2 score corresponding to 91 is 0.5906530709693818\n",
      "r2 score corresponding to 92 is 0.6036864378679381\n",
      "r2 score corresponding to 93 is 0.6133196500071614\n",
      "r2 score corresponding to 94 is 0.6045844105395015\n",
      "r2 score corresponding to 95 is 0.6058108614194762\n",
      "r2 score corresponding to 96 is 0.5939950930619886\n",
      "r2 score corresponding to 97 is 0.5983359840332025\n",
      "r2 score corresponding to 98 is 0.6055194509628175\n",
      "r2 score corresponding to 99 is 0.6160456359554072\n",
      "max r2 score corresponding to 52 is 0.6278766975761043\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "#Checking max r2 score when we use Ridge\n",
    "enr=ElasticNet(alpha=0.01)\n",
    "r_state=maxr2_score(enr,x,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 496,
   "metadata": {},
   "outputs": [],
   "source": [
    "#We tried all methods and till now Gradient Boosting Regression is the best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean r2 score for Gradient Boosting Regression: 0.9107855536617041\n",
      "Standard Deviation in r2 score for Gradient Boosting Regression: 0.006188592184029789\n"
     ]
    }
   ],
   "source": [
    "#Random state corresponding to highest r2 score is 52\n",
    "x_train,x_test,y_train,y_test=train_test_split(x,y,random_state=52,test_size=0.20)\n",
    "gbr=GradientBoostingRegressor(learning_rate=1,n_estimators=500)\n",
    "gbr.fit(x_train,y_train)\n",
    "y_pred=gbr.predict(x_test)\n",
    "print(\"Mean r2 score for Gradient Boosting Regression:\",cross_val_score(gbr,x,y,cv=5,scoring='r2').mean())\n",
    "print('Standard Deviation in r2 score for Gradient Boosting Regression:',cross_val_score(gbr,x,y,cv=5,scoring='r2').std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE is: 0.14459559524719098\n",
      "r2 score is: 0.9230171333279182\n"
     ]
    }
   ],
   "source": [
    "#Finding RMSE and r2 score using sklearn.metrics\n",
    "import numpy as np\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "print(\"RMSE is:\",np.sqrt(mean_squared_error(y_test,y_pred)))\n",
    "print(\"r2 score is:\",r2_score(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 499,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['project12_flight.pkl']"
      ]
     },
     "execution_count": 499,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Saving the modelas a pickle in a file\n",
    "from sklearn.externals import joblib\n",
    "joblib.dump(gbr,\"project12_flight.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
